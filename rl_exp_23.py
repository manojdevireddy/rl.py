# -*- coding: utf-8 -*-
"""rl exp 23.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CiO-H7ePZhnYxYh7a__LkCjlflEK-7zy
"""

import numpy as np
import tensorflow as tf
import gym

# Define the environment
class GridWorldEnv:
    def __init__(self):
        self.grid_size = 5
        self.num_actions = 4  # 4 possible actions: up, down, left, right
        self.start_state = (0, 0)
        self.goal_state = (self.grid_size - 1, self.grid_size - 1)
        self.obstacles = [(1, 1), (2, 2), (3, 3)]

    def reset(self):
        self.state = self.start_state
        return self.state

    def step(self, action):
        if action == 0:  # Up
            next_state = (self.state[0] - 1, self.state[1])
        elif action == 1:  # Down
            next_state = (self.state[0] + 1, self.state[1])
        elif action == 2:  # Left
            next_state = (self.state[0], self.state[1] - 1)
        elif action == 3:  # Right
            next_state = (self.state[0], self.state[1] + 1)

        if next_state[0] < 0 or next_state[0] >= self.grid_size or \
           next_state[1] < 0 or next_state[1] >= self.grid_size or \
           next_state in self.obstacles:
            # Invalid move, stay in the same state
            reward = -1
            next_state = self.state
        elif next_state == self.goal_state:
            # Reached the goal
            reward = 10
        else:
            reward = 0

        self.state = next_state
        return next_state, reward

# Define the policy network
class PolicyNetwork(tf.keras.Model):
    def __init__(self, num_actions):
        super(PolicyNetwork, self).__init__()
        self.dense1 = tf.keras.layers.Dense(32, activation='relu')
        self.dense2 = tf.keras.layers.Dense(num_actions, activation='softmax')

    def call(self, state):
        x = self.dense1(state)
        return self.dense2(x)

# Proximal Policy Optimization (PPO) agent
class PPOAgent:
    def __init__(self, env, policy_network, optimizer, epochs=10, epsilon=0.2):
        self.env = env
        self.policy_network = policy_network
        self.optimizer = optimizer
        self.epochs = epochs
        self.epsilon = epsilon

    def get_action(self, state):
        state = tf.convert_to_tensor([state], dtype=tf.float32)
        probs = self.policy_network(state)
        action = tf.random.categorical(tf.math.log(probs), 1)[0, 0]
        return action.numpy()

    def update(self, states, actions, advantages, old_probs):
        with tf.GradientTape() as tape:
            new_probs = tf.reduce_sum(
                tf.one_hot(actions, self.env.num_actions) * self.policy_network(states), axis=1)
            ratio = new_probs / old_probs
            clipped_ratio = tf.clip_by_value(ratio, 1-self.epsilon, 1+self.epsilon)
            surrogate1 = ratio * advantages
            surrogate2 = clipped_ratio * advantages
            loss = -tf.reduce_mean(tf.minimum(surrogate1, surrogate2))
        grads = tape.gradient(loss, self.policy_network.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.policy_network.trainable_variables))
        return loss

    def train(self, episodes):
        for episode in range(episodes):
            states = []
            actions = []
            rewards = []
            old_probs = []

            state = self.env.reset()
            while True:
                states.append(state)
                action = self.get_action(state)
                actions.append(action)
                old_prob = self.policy_network(tf.convert_to_tensor([state], dtype=tf.float32))[0, action].numpy()
                old_probs.append(old_prob)
                next_state, reward = self.env.step(action)
                rewards.append(reward)

                if next_state == self.env.goal_state:
                    break

                state = next_state

            rewards = np.array(rewards)
            advantages = self.compute_advantages(rewards)

            states = tf.convert_to_tensor(states, dtype=tf.float32)
            actions = tf.convert_to_tensor(actions, dtype=tf.int32)
            old_probs = tf.convert_to_tensor(old_probs, dtype=tf.float32)
            advantages = tf.convert_to_tensor(advantages, dtype=tf.float32)

            for _ in range(self.epochs):
                loss = self.update(states, actions, advantages, old_probs)

            print(f"Episode {episode + 1}, Total Reward: {np.sum(rewards)}, Loss: {loss}")

    def compute_advantages(self, rewards):
        advantages = np.zeros_like(rewards, dtype=np.float32)
        advantage = 0
        for t in reversed(range(len(rewards))):
            if rewards[t] != 0:
                advantage = 0
            advantage = advantage * 0.99 + rewards[t]
            advantages[t] = advantage
        return (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-8)

# Main program
if __name__ == "__main__":
    env = GridWorldEnv()
    policy_network = PolicyNetwork(env.num_actions)
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
    agent = PPOAgent(env, policy_network, optimizer)
    agent.train(episodes=100)

pip install stable-baselines3

import gym
from stable_baselines3 import PPO

# Define a custom gym environment
class CustomEnv(gym.Env):
    def __init__(self):
        self.observation_space = gym.spaces.Discrete(16)  # 4x4 grid
        self.action_space = gym.spaces.Discrete(4)  # Up, Down, Left, Right
        self.state = 0  # Initial state
        self.goal_state = 15  # Goal state
        self.obstacle_states = [5, 7, 11]  # Obstacle states

    def reset(self):
        self.state = 0
        return self.state

    def step(self, action):
        if action == 0:  # Up
            new_state = self.state - 4 if self.state >= 4 else self.state
        elif action == 1:  # Down
            new_state = self.state + 4 if self.state < 12 else self.state
        elif action == 2:  # Left
            new_state = self.state - 1 if self.state % 4 != 0 else self.state
        elif action == 3:  # Right
            new_state = self.state + 1 if self.state % 4 != 3 else self.state

        if new_state in self.obstacle_states:
            reward = -1
            done = False
        elif new_state == self.goal_state:
            reward = 1
            done = True
        else:
            reward = 0
            done = False

        self.state = new_state
        return self.state, reward, done, {}

# Create the environment
env = CustomEnv()

# Instantiate the PPO agent
model = PPO("MlpPolicy", env, verbose=1)

# Train the agent
model.learn(total_timesteps=10000)

# Evaluate the trained agent
obs = env.reset()
for _ in range(20):
    action, _states = model.predict(obs, deterministic=True)
    obs, rewards, dones, info = env.step(action)
    env.render()
    if dones:
        break

env.close()